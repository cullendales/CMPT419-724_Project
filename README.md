# Changes From the proposal
We switched the evaluation metric to BERTScore because it is easier to obtain and computes similarity scores between predicted and reference texts based on their contextual embeddings from a pre-trained BERT model. After reviewing the generated results, we determined that BERTScore is the most suitable metric for our project.

Human evaluation was conducted using two metrics instead of one: "gesture_score" and "meaning_score ." This approach helps assess whether the model fails to capture the gesture shape entirely or simply misses the meaning in a specific culture, despite recognizing the gesture.

Additionally, we computed the most common words in the output of each model, segmented by culture, to investigate potential biases towards specific gestures within a given culture.

As LLaVA was too large to run on our machines, results were generated using the website https://huggingface.co/spaces/badayvedat/LLaVA with the parameters: Temperature = 0.2, Top P = 0.7, and Max Tokens = 512.

cogVLM was changed to Perplexity-AI as cogVLM was too large for any of our machines to handle and we had access to Perplexity AI through tokens.

# Structure

### Dataset Folder

The Dataset folder contains hand-annotated datasets used as input for the models. Each subfolder represents gestures from three different cultures and includes a labels.txt file containing annotated labels for each image.

### Model Evaluation Folder

The Model Evaluation folder includes analysis conducted on the output of the models. It contains:

- **comparison.py**: Generates plots comparing accuracy and BERT scores across three models.
- **plots folder**: Contains plots comparing the results of all models.

### Subfolders (Llava_Evaluation, Perplexity_Evaluation, and BLIP2_Evaluation)

Each subfolder includes:

- **Txt files**: Output of corresponding models with columns "meaning_score" and "gesture_score," evaluated by human annotators.
- **compute_metrics.py**: Computes the number of correct predictions based on human evaluations and calculates BERT scores between assigned and predicted labels.
- **most_common.py**: Computes the most common n-grams in result columns.
- **culture_score_summary.csv**: Scores by culture generated by compute_metrics.py.
- **bert_score_summary.csv**: BERT scores generated by compute_metrics.py.
- **common_phrases_blip2_by_culture.csv**: Common phrases by culture generated by most_common.py.

### Run_Models Folder

The Run_Models folder contains three subfolders, each with scripts used to run corresponding models. 
The folders also contain the results obtained from running the models.


**Note:**  
Paths in all scripts must be manually set to the appropriate folders.
